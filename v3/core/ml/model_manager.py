"""
–ú–µ–Ω–µ–¥–∂–µ—Ä ML –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤.
–£–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ–º, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏.
"""

import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple, Union
import pandas as pd
import numpy as np

from .storage import ml_storage
from .predictive_models import LSTMPredictor, GRUPredictor, ModelConfig
from .sentiment_analysis import NewsSentimentAnalyzer, SentimentConfig
from .clustering import StockClusterer, ClusteringConfig
from .ensemble_methods import EnsemblePredictor, EnsembleConfig

logger = logging.getLogger(__name__)


class MLModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä ML –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤."""
    
    def __init__(self, db_path: str = "stock_data.db"):
        self.db_path = db_path
        self.storage = ml_storage
        
        # –ö—ç—à –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.trained_models = {}
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        self.timeframe_configs = {
            '1d': {
                'sequence_length': 60,
                'hidden_size': 50,
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001,
                'validation_split': 0.2
            },
            '1h': {
                'sequence_length': 168,  # 1 –Ω–µ–¥–µ–ª—è —á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                'hidden_size': 64,
                'epochs': 150,
                'batch_size': 64,
                'learning_rate': 0.0008,
                'validation_split': 0.2
            },
            '1m': {
                'sequence_length': 1440,  # 1 –¥–µ–Ω—å –º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                'hidden_size': 128,
                'epochs': 200,
                'batch_size': 128,
                'learning_rate': 0.0005,
                'validation_split': 0.15
            },
            '1s': {
                'sequence_length': 3600,  # 1 —á–∞—Å —Å–µ–∫—É–Ω–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                'hidden_size': 256,
                'epochs': 300,
                'batch_size': 256,
                'learning_rate': 0.0003,
                'validation_split': 0.1
            }
        }
    
    def get_or_train_model(self, symbol: str, model_type: str = 'lstm', 
                          timeframe: str = '1d', force_retrain: bool = False) -> Tuple[Any, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏–ª–∏ –æ–±—É—á–∏—Ç—å –Ω–æ–≤—É—é.
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∞–∫—Ü–∏–∏
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('lstm', 'gru', 'ensemble', 'sentiment')
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º ('1d', '1h', '1m', '1s')
            force_retrain: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
            
        Returns:
            Tuple[–º–æ–¥–µ–ª—å, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ]
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if not force_retrain:
            model, metadata = self.storage.get_model(symbol, model_type, timeframe)
            if model is not None:
                logger.debug(f"Using cached model for {symbol}_{model_type}_{timeframe}")
                return model, metadata
        
        # –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        logger.info(f"Training new {model_type} model for {symbol} ({timeframe})")
        return self._train_model(symbol, model_type, timeframe)
    
    def predict_price_movement(self, symbol: str, timeframe: str = '1d', 
                             days_ahead: int = 1, use_cache: bool = True) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã.
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –∞–∫—Ü–∏–∏
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö
            days_ahead: –ù–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if use_cache:
            cached_prediction = self.storage.get_prediction(symbol, 'price', timeframe)
            if cached_prediction:
                logger.debug(f"Using cached price prediction for {symbol}")
                return {
                    'prediction': cached_prediction['prediction'],
                    'confidence': cached_prediction['confidence'],
                    'cached': True,
                    'cache_date': cached_prediction['date']
                }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            historical_data = self._get_stock_data_from_db(symbol, timeframe)
            if historical_data.empty:
                return {'error': f'No historical data available for {symbol}'}
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model, metadata = self.get_or_train_model(symbol, 'lstm', timeframe)
            
            if model is None:
                return {'error': f'Failed to train model for {symbol}'}
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction_result = model.predict(historical_data.tail(100))
            
            prediction = prediction_result.predictions[-1] if len(prediction_result.predictions) > 0 else 0
            confidence = prediction_result.confidence
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self.storage.save_prediction(symbol, 'price', prediction, confidence, timeframe)
            
            return {
                'prediction': prediction,
                'confidence': confidence,
                'model_metrics': {
                    'mse': prediction_result.mse,
                    'mae': prediction_result.mae,
                    'rmse': prediction_result.rmse
                },
                'training_metrics': metadata,
                'cached': False
            }
            
        except Exception as e:
            logger.error(f"Error predicting price movement for {symbol}: {e}")
            return {'error': str(e)}
    
    def analyze_sentiment(self, symbol: str, timeframe: str = '1d', 
                         use_cache: bool = True) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –¥–ª—è —Å–∏–º–≤–æ–ª–∞."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache:
            cached_sentiment = self.storage.get_prediction(symbol, 'sentiment', timeframe)
            if cached_sentiment:
                logger.debug(f"Using cached sentiment for {symbol}")
                return {
                    'sentiment': cached_sentiment['prediction'],
                    'confidence': cached_sentiment['confidence'],
                    'cached': True,
                    'cache_date': cached_sentiment['date']
                }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
            news_data = self._get_news_data_from_db(symbol, timeframe)
            
            if news_data.empty:
                return {'error': f'No news data available for {symbol}'}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            sentiment_config = SentimentConfig()
            sentiment_analyzer = NewsSentimentAnalyzer(sentiment_config)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            texts = []
            for _, row in news_data.iterrows():
                text_parts = []
                if pd.notna(row.get('title')):
                    text_parts.append(str(row['title']))
                if pd.notna(row.get('content')):
                    text_parts.append(str(row['content']))
                if text_parts:
                    texts.append(' '.join(text_parts))
            
            if not texts:
                return {'error': 'No valid news texts found'}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            sentiment_results = sentiment_analyzer.analyze_batch(texts[:10])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            sentiments = [r.sentiment for r in sentiment_results]
            confidences = [r.confidence for r in sentiment_results]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
            sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
            for sentiment in sentiments:
                sentiment_counts[sentiment] += 1
            
            overall_sentiment = max(sentiment_counts, key=sentiment_counts.get)
            avg_confidence = np.mean(confidences) if confidences else 0.5
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self.storage.save_prediction(symbol, 'sentiment', 
                                       float(sentiment_counts[overall_sentiment] / len(sentiments)), 
                                       avg_confidence, timeframe)
            
            return {
                'sentiment': overall_sentiment,
                'confidence': avg_confidence,
                'sentiment_distribution': sentiment_counts,
                'news_count': len(texts),
                'cached': False
            }
            
        except Exception as e:
            logger.error(f"Error analyzing sentiment for {symbol}: {e}")
            return {'error': str(e)}
    
    def cluster_stocks(self, symbols: List[str], timeframe: str = '1d', 
                      n_clusters: int = 5) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∞–∫—Ü–∏–π."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
            all_data = []
            valid_symbols = []
            
            for symbol in symbols:
                data = self._get_stock_data_from_db(symbol, timeframe)
                if not data.empty:
                    all_data.append(data)
                    valid_symbols.append(symbol)
            
            if not all_data:
                return {'error': 'No valid data for clustering'}
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            combined_data = pd.concat(all_data, ignore_index=True)
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
            clustering_config = ClusteringConfig(n_clusters=n_clusters)
            clusterer = StockClusterer(clustering_config)
            
            clustering_result = clusterer.cluster_stocks(combined_data)
            
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ —Å–∏–º–≤–æ–ª–æ–≤ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
            symbol_clusters = {}
            for i, symbol in enumerate(valid_symbols):
                if i < len(clustering_result.cluster_labels):
                    symbol_clusters[symbol] = int(clustering_result.cluster_labels[i])
            
            return {
                'clusters': symbol_clusters,
                'n_clusters': clustering_result.n_clusters,
                'silhouette_score': clustering_result.silhouette_score,
                'symbols_processed': len(valid_symbols)
            }
            
        except Exception as e:
            logger.error(f"Error clustering stocks: {e}")
            return {'error': str(e)}
    
    def get_ensemble_prediction(self, symbol: str, timeframe: str = '1d') -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            historical_data = self._get_stock_data_from_db(symbol, timeframe)
            if historical_data.empty:
                return {'error': f'No historical data available for {symbol}'}
            
            # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
            ensemble_config = EnsembleConfig()
            ensemble = EnsemblePredictor(ensemble_config)
            
            # –û–±—É—á–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å
            ensemble_result = ensemble.predict(historical_data)
            
            return {
                'prediction': ensemble_result.predictions[-1] if len(ensemble_result.predictions) > 0 else 0,
                'confidence': ensemble_result.confidence,
                'model_type': 'ensemble',
                'individual_predictions': getattr(ensemble_result, 'individual_predictions', {}),
                'weights': getattr(ensemble_result, 'weights', {})
            }
            
        except Exception as e:
            logger.error(f"Error getting ensemble prediction for {symbol}: {e}")
            return {'error': str(e)}
    
    def get_model_status(self, timeframe: Optional[str] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
            models_df = self.storage.get_active_models(timeframe)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            timeframe_stats = {}
            for tf in ['1d', '1h', '1m', '1s']:
                tf_models = models_df[models_df['timeframe'] == tf] if not models_df.empty else pd.DataFrame()
                timeframe_stats[tf] = {
                    'total_models': len(tf_models),
                    'symbols': tf_models['symbol'].nunique() if not tf_models.empty else 0,
                    'avg_accuracy': tf_models['accuracy'].mean() if not tf_models.empty and 'accuracy' in tf_models.columns else 0.0,
                    'latest_training': tf_models['training_date'].max() if not tf_models.empty and 'training_date' in tf_models.columns else None
                }
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_models = len(models_df)
            total_symbols = models_df['symbol'].nunique() if not models_df.empty else 0
            avg_accuracy = models_df['accuracy'].mean() if not models_df.empty and 'accuracy' in models_df.columns else 0.0
            
            return {
                'total_models': total_models,
                'total_symbols': total_symbols,
                'average_accuracy': avg_accuracy,
                'timeframe_stats': timeframe_stats,
                'models': models_df.to_dict('records') if not models_df.empty else []
            }
            
        except Exception as e:
            logger.error(f"Error getting model status: {e}")
            return {'error': str(e)}
    
    def cleanup_expired_models(self) -> int:
        """–û—á–∏—Å—Ç–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–æ–¥–µ–ª–∏."""
        return self.storage.cleanup_expired_models()
    
    def _train_model(self, symbol: str, model_type: str, timeframe: str) -> Tuple[Any, Dict[str, Any]]:
        """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            historical_data = self._get_stock_data_from_db(symbol, timeframe)
            if historical_data.empty:
                raise ValueError(f'No historical data available for {symbol}')
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            config_params = self.timeframe_configs.get(timeframe, self.timeframe_configs['1d'])
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
            if model_type == 'lstm':
                model_config = ModelConfig(
                    sequence_length=config_params['sequence_length'],
                    hidden_size=config_params['hidden_size'],
                    epochs=config_params['epochs'],
                    batch_size=config_params['batch_size'],
                    learning_rate=config_params['learning_rate'],
                    validation_split=config_params['validation_split']
                )
                model = LSTMPredictor(model_config)
            elif model_type == 'gru':
                model_config = ModelConfig(
                    sequence_length=config_params['sequence_length'],
                    hidden_size=config_params['hidden_size'],
                    epochs=config_params['epochs'],
                    batch_size=config_params['batch_size'],
                    learning_rate=config_params['learning_rate'],
                    validation_split=config_params['validation_split']
                )
                model = GRUPredictor(model_config)
            else:
                raise ValueError(f'Unsupported model type: {model_type}')
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            start_time = datetime.now()
            training_result = model.train(historical_data)
            training_duration = (datetime.now() - start_time).total_seconds()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            training_metrics = {
                'accuracy': training_result.get('final_val_loss', 0.0),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ proxy –¥–ª—è accuracy
                'mse': training_result.get('final_train_loss', 0.0),
                'mae': training_result.get('final_train_loss', 0.0),
                'rmse': np.sqrt(training_result.get('final_train_loss', 0.0)),
                'data_points': len(historical_data),
                'epochs_trained': training_result.get('epochs_trained', 0),
                'training_duration': training_duration,
                'sequence_length': config_params['sequence_length'],
                'hidden_size': config_params['hidden_size'],
                'features_used': ['open', 'high', 'low', 'close', 'volume']  # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            metadata = self.storage.save_model(symbol, model, model_type, timeframe, training_metrics)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
            self._record_training_history(symbol, model_type, timeframe, training_metrics, 'completed')
            
            logger.info(f"Model {symbol}_{model_type}_{timeframe} trained successfully")
            return model, metadata
            
        except Exception as e:
            logger.error(f"Error training model {symbol}_{model_type}_{timeframe}: {e}")
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ –∏—Å—Ç–æ—Ä–∏—é
            self._record_training_history(symbol, model_type, timeframe, {}, 'failed', str(e))
            raise
    
    def _get_stock_data_from_db(self, symbol: str, timeframe: str = '1d') -> pd.DataFrame:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∞–∫—Ü–∏–π –∏–∑ –ë–î."""
        import sqlite3
        from pathlib import Path
        
        print(f"    üìä [ML_DATA] –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}...")
        
        db_path = "stock_data.db"
        if not Path(db_path).exists():
            print(f"    ‚ùå [ML_DATA] {symbol}: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return pd.DataFrame()
        
        conn = sqlite3.connect(db_path)
        try:
            # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –¥–Ω–µ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            query = """
                SELECT datetime as date, open, high, low, close, volume
                FROM data_1d 
                WHERE symbol = ?
                ORDER BY datetime DESC
                LIMIT 1000
            """
            
            df = pd.read_sql_query(query, conn, params=(symbol,))
            
            if df.empty:
                print(f"    ‚ùå [ML_DATA] {symbol}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ data_1d")
                return df
            else:
                print(f"    ‚úÖ [ML_DATA] {symbol}: –ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df = self._add_technical_indicators(df)
            
            print(f"    ‚úÖ [ML_DATA] {symbol}: –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∏—Ç–æ–≥–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return df
            
        except Exception as e:
            print(f"    ‚ùå [ML_DATA] {symbol}: –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö - {e}")
            logger.error(f"Error getting stock data for {symbol}: {e}")
            return pd.DataFrame()
        finally:
            conn.close()
    
    def _get_news_data_from_db(self, symbol: str, timeframe: str = '1d') -> pd.DataFrame:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î."""
        import sqlite3
        from pathlib import Path
        
        db_path = "stock_data.db"
        if not Path(db_path).exists():
            return pd.DataFrame()
        
        conn = sqlite3.connect(db_path)
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
            days_back = 7 if timeframe == '1d' else 1
            
            # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND (name LIKE '%news%' OR name LIKE '%article%')")
            news_tables = cursor.fetchall()
            
            if not news_tables:
                return pd.DataFrame()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
            news_table = news_tables[0][0]
            
            # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
            query = f"""
                SELECT title, content, date
                FROM {news_table}
                WHERE date >= date('now', '-{days_back} days')
                ORDER BY date DESC
                LIMIT 100
            """
            
            df = pd.read_sql_query(query, conn)
            return df
            
        except Exception as e:
            logger.error(f"Error getting news data: {e}")
            return pd.DataFrame()
        finally:
            conn.close()
    
    def _add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã."""
        try:
            # –ü—Ä–æ—Å—Ç—ã–µ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            df['sma_20'] = df['close'].rolling(window=20).mean()
            df['sma_50'] = df['close'].rolling(window=50).mean()
            
            # EMA
            df['ema_12'] = df['close'].ewm(span=12).mean()
            df['ema_26'] = df['close'].ewm(span=26).mean()
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # MACD
            df['macd'] = df['ema_12'] - df['ema_26']
            
            # Bollinger Bands
            bb_period = 20
            bb_std = 2
            df['bb_middle'] = df['close'].rolling(window=bb_period).mean()
            bb_std_val = df['close'].rolling(window=bb_period).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std_val * bb_std)
            df['bb_lower'] = df['bb_middle'] - (bb_std_val * bb_std)
            
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
            df = df.ffill().fillna(0)
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding technical indicators: {e}")
            return df
    
    def _record_training_history(self, symbol: str, model_type: str, timeframe: str, 
                               training_metrics: Dict[str, Any], status: str, 
                               error_message: Optional[str] = None) -> None:
        """–ó–∞–ø–∏—Å–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ –ë–î."""
        import sqlite3
        
        conn = sqlite3.connect(self.db_path)
        try:
            conn.execute("""
                INSERT INTO ml_training_history
                (symbol, model_type, timeframe, training_start, training_end,
                 duration_seconds, data_points, epochs_trained, final_accuracy,
                 final_loss, hyperparameters, training_status, error_message)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                symbol, model_type, timeframe,
                training_metrics.get('training_start', datetime.now().isoformat()),
                training_metrics.get('training_end', datetime.now().isoformat()),
                training_metrics.get('training_duration', 0.0),
                training_metrics.get('data_points', 0),
                training_metrics.get('epochs_trained', 0),
                training_metrics.get('accuracy', 0.0),
                training_metrics.get('mse', 0.0),
                '{}',  # hyperparameters
                status,
                error_message
            ))
            conn.commit()
        finally:
            conn.close()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
ml_model_manager = MLModelManager()
